{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import re, codecs, nltk, pickle\n",
    "# nltk.download()\n",
    "# Notice the pop-up window --> collections (tab) --> popular\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate hebrew sentence to english\n",
    "def translate_sentence(s):\n",
    "    translator = Translator()\n",
    "    translated_message = translator.translate(s)\n",
    "    return translated_message.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chat(chat):\n",
    "    clean_chat = []\n",
    "    for message in chat:\n",
    "        # Remove non-letters        \n",
    "        letters_only = re.sub(\"[^א-ת]\", \" \", message) \n",
    "        #\n",
    "        clean_s = \" \".join(letters_only.split())\n",
    "        trans_s = translate_sentence(clean_s)\n",
    "        \n",
    "        # Convert to lower case, split into individual words\n",
    "        words = trans_s.lower().split()   \n",
    "        #\n",
    "        # In Python, searching a set is much faster than searching\n",
    "        #   a list, so convert the stop words to a set\n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        # \n",
    "        # steeming using PorterStemmer\n",
    "        porter = nltk.PorterStemmer()\n",
    "        after_stemmnig = [porter.stem(w) for w in words]\n",
    "        # Remove stop words\n",
    "        meaningful_words = [w for w in after_stemmnig if not w in stops]   \n",
    "        #\n",
    "        # Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        clean_chat.append( \" \".join(meaningful_words)) \n",
    "    return clean_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read messages from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read chat as message list from file\n",
    "man_chats = []\n",
    "women_chats = []\n",
    "\n",
    "with codecs.open(\"man.txt\", encoding='utf-8') as fp:   # Unpickling\n",
    "    man_chats = fp.read().splitlines() \n",
    "with codecs.open(\"woman.txt\", encoding='utf-8') as fp:   # Unpickling\n",
    "    women_chats = fp.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of man chats = 18643, length of woman chats = 5418\n"
     ]
    }
   ],
   "source": [
    "print ('length of man chats = {}, length of woman chats = {}'.format(len(man_chats),len(women_chats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the minimum lebgth\n",
    "number_of_sentence = min(len(man_chats),len(women_chats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "l = number_of_sentence\n",
    "slim_women_chats = women_chats[:l]\n",
    "\n",
    "while len(man_chats)>number_of_sentence:\n",
    "  man_chats.remove(random.choice(list(man_chats)))\n",
    "slim_man_chats = man_chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of man chats = 5418, length of woman chats = 5418\n"
     ]
    }
   ],
   "source": [
    "print ('length of man chats = {}, length of woman chats = {}'.format(len(slim_man_chats),len(slim_women_chats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean and translate chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# women\n",
    "clean_message_list_women = clean_chat(slim_women_chats)\n",
    "# save the list\n",
    "with open('translated_woman.pkl', 'wb') as fid:\n",
    "    pickle.dump(clean_message_list_women, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# man\n",
    "clean_message_list_man = clean_chat(slim_man_chats)\n",
    "# save the list\n",
    "with open('translated_man.pkl', 'wb') as fid:\n",
    "    pickle.dump(clean_message_list_man, fid)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_message_list = clean_message_list_women + clean_message_list_man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create classification DF ('man' or 'women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = []\n",
    "for _ in range(l):\n",
    "    classification.append('women')\n",
    "for _ in range(l):\n",
    "    classification.append('man')\n",
    "class_df = pd.DataFrame({'Gender' : np.array(classification)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBOW(clean_message_list):\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 1000)\n",
    "    train_data_features = vectorizer.fit_transform(clean_message_list)\n",
    "    \n",
    "    \n",
    "    voc = vectorizer.get_feature_names()\n",
    "    # save the dictionary for future use\n",
    "    with open('voc.pkl', 'wb') as fid:\n",
    "        pickle.dump(voc, fid)  \n",
    "        \n",
    "    return train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = createBOW(clean_message_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split to train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ran = np.random.rand(len(train_data_features)) < 0.8\n",
    "train_sequence = train_data_features[ran]\n",
    "test_sequence = train_data_features[~ran]\n",
    "train_class = class_df.loc[ran, 'Gender']\n",
    "test_class = class_df.loc[~ran, 'Gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Base\n",
    "The Naive Bayes Classifier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high. Despite its simplicity, Naive Bayes can often outperform more sophisticated classification methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6708576560395151"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize a Naive Bayes classifier.\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Fit the Naive Bayes to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "nb = nb.fit( train_sequence, train_class )\n",
    "\n",
    "# Evaluate accuracy best on the test set\n",
    "naive_bayes = nb.score(test_sequence,test_class)\n",
    "\n",
    "naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC\n",
    "Support Vector Machines are based on the concept of decision planes that define decision boundaries. A decision plane is one that separates between a set of objects having different class memberships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7503367759317467"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Initialize a svc classifier with gamma = 0.001, C = 100, degree = 3\n",
    "svc = svm.SVC(gamma = 0.001, C = 100, degree = 3)\n",
    "\n",
    "# Fit the svc to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "svc = svc.fit( train_sequence, train_class )\n",
    "\n",
    "# Evaluate accuracy best on the test set\n",
    "support_vector = svc.score(test_sequence,test_class)\n",
    "\n",
    "support_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare between to models\n",
    "as we see SVC give us better score so we decide to take this model to step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the trained model for step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classifier\n",
    "with open('classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(svc, fid)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
